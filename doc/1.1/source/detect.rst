************************************
Detection
************************************


Collection of Anomaly Detection Algorithm
------------------------------------

It provides a collection of Internet Anomaly Detection algorithms proposed by
several researchers. Now it contains:

    1. Stochastic Anomaly Detector using Large Deviation Theory
    2. Deterministic Temporal Anomaly Detector using Support Vector Machine
    3. Deterministic Flow by Flow Detector using Support Vector Machine
    4. Anomaly Detection Techniques using ART theory

SADIT is not limited to the four algorithms listed above, its utimate goal is to
become a standard collection of a variety of internet anomaly detection
algorithms.

All the detection algorithms locates in the *ROOT/Detector* folder:

    - **SVMDetector.py** contains two SVM based anomaly detection algorithmes 
      1. SVM Temporal Detector and 2. SVM Flow by Flow Detector.
    - **StoDetector.py** contains two anomaly detection algorithms based on
      Large Deviation Theory.

Parameters for Detectors
-------------------------------------

.. code-block:: python

    DETECTOR_DESC = dict(
            interval=20,
            win_size=200,
            win_type='time', # 'time'|'flow'
            fr_win_size=100, # window size for estimation of flow rate
            false_alarm_rate = 0.001,
            unified_nominal_pdf = False, # used in sensitivities analysis
            fea_option = {'dist_to_center':2, 'flow_size':2, 'cluster':3},
            ano_ana_data_file = ANO_ANA_DATA_FILE,
            normal_rg = None, # range for normal traffic, if it none, use the
            whole traffic
            detector_type = 'mfmb', # can be anyone specfied by **detector_map** in *Detector/API.py*
            max_detect_num = None, # the maximum detection number the detector will run
            data_handler = 'fs', # specify the data handler, can be any value defined in 
            data_handler_map in Detector/API.py

            #### only for SVM approach #####
            # gamma = 0.01,
            )

Want to implement your algorithm?
=====================================

Use the labeled flow records generator in fs simulator
-------------------------------------
The generated flows will be the *ROOT/Simulator* folder. The flows end with
*_flow.txt*, for example, n0_flow.txt is the network flows trough node 0. File
start with *abnormal_* is the exported abnormal flows correspondingly.

**A typical line is**
    textexport n0 1348412129.925416 1348412129.925416 1348412130.070733 10.0.7.4:80->10.0.8.5:53701 tcp 0x0 n1 5 4215 FSA

**line format**
    prefix nodename time flow_start_time flow_end_time src_ip:src_port->dst_ip:dst_port protocol payload destname unknown flowsize unknown

After finishing your detection algorihms, the last thing you need to do is to
add the corresponding class name to **detector_map** in *ROOT/Detector/API.py*.
After that you will be able to use your detection algorithm. You can use
**Compare** experiment to compare with other algorithm or **Eval** algorithm to
Evaluate your algorithm. You can also implement new experiment to play with your
new algorithm.

..
    Then you can implement
    your own experiment to compare your algorithms with existing algorithms in
    SADIT. Look at the sample examples in  *ROOT/Experiemnt/* folder. Your can run
    your experiment by typing ::
        ./run.py -e <Your Experiment Name>

Use Other flow records
-------------------------------------
SADIT does not only support the text output format of fs simulator, but also
several other types of flow data. The handler classes locate in the
*DataHandler.py* and *DataHandler_xflow.py*
**DataFile**
    - **PreloadHardDistFile** hard disk flow file generated by fs-simulator. It
      will preload all the flow file into memory, so it cannot deal with flow
      file larger than your memery
    - **PreloadHardDistFile_pcap2netflow** hard disk flow file generated by
      `pcap2netflow <https://bitbucket.org/hbhzwj/pcap2netflow/src>`_ tool(the
      format of `flowd-reader <http://www.mindrot.org/projects/softflowd/>`_)
    - **PreloadHardDistFile_xflow**, hard disk flow file generated by xflow tool
    - **SQLFile_SperottoIPOM2009**, labeled data stored in mysql server provided
      by `simpleweb.org <http://traces.simpleweb.org/traces/netflow/netflow2/>`_


The following class is the abstract base class for all data files

.. code-block:: python

    class Data(object):
        """virtual base class for data. Data class deals with any implementation
        details of the data. it can be a file, a sql data base, and so on, as long
        as it support the pure virtual methods defined here.
        """
        def get_fea_slice(self, rg=None, rg_type=None):
            """ get a slice of feature
            - **rg** is the range for the slice
            - **rg_type** is the type for range, it can be ['flow'|'time']
            """
            abstract_method()

        def get_max(self, fea, rg=None, rg_type=None):
            """ get the max value of feature during a range
            - **fea** is a list of feature name
            - **rg** is the range
            - **rg_type** is the range type
            the output is the a list of element which contains the max
            value of the feature in **fea**
            """
            abstract_method()
        def get_min(self, fea, rg=None, rg_time=None):
            """ get min value of feature within a range. see **get_max** for
            the meaning of the parameters
            """
            abstract_method()

It defines the operation we can do with data files. The four
algorithmes we implemented requires **get_fea_slice**, **get_max** and
**get_min** operations. You can implement more operations, but at least
implement these three operations.

Optionally, you can implement a handler class that will manipulate the
DataFile and and some useful quantities that may be useful to you algorithms.
For example, we implemented **HardDiskFileHandler** with get_em() function to
get probability distribution of the flows, which is useful for the stochastic
approaches. If you just need the raw data, you can define simple handler class
with data file object you want to use as member variable.

Then you just need to add your data_handler to **data_handler_handle_map**
defined in *ROOT/Detector/API.py*

..  **DataFileHandler**
   [HardDiskFileHandler]: 
   [HardDiskFileHandler_pcap2netflow]: h    
     [HardDiskFileHandler_xflow]: 
   [SQLDataFileHandler_SperottoIPOM2009]: 
..  you want to work with new data format, here are some tips:


